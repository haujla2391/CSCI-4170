{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haujla2391/CSCI-4170/blob/main/01_data_and_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A"
      ],
      "metadata": {
        "id": "lUkuOa_2fa5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "e_DKURQJfcI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasheet:\n",
        "\n",
        "Dataset name + link: https://www.kaggle.com/code/asmahwimli/human-activity-recognition\n",
        "\n",
        "License/terms:\n",
        "* Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.  This dataset is distributed AS-IS and no responsibility implied or explicit can be addressed to the authors or their institutions for its use or misuse. Any commercial use is prohibited.\n",
        "\n",
        "* Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013\n",
        "\n",
        "* Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223.\n",
        "\n",
        "* Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Catal�. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n",
        "\n",
        "Prediction task + target definition:\n",
        "* The task is to classify human activities using data collected from a smartphone's accelerometer and gyroscope.\n",
        "* Target: Activity Label (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying)\n",
        "\n",
        "Units of prediction (window length T, channels C, sampling rate):\n",
        "* T - 128 samples (time steps) per row\n",
        "* C - 9 channels total\n",
        "* Sampling rate - 50 Hz\n",
        "\n",
        "Label set and class balance (counts per class):\n",
        "* Walking - 1,722\n",
        "* Walking Upstairs - 1,544\n",
        "* Walking Downstairs - 1,406\n",
        "* Sitting - 1,777\n",
        "* Standing - 1,906\n",
        "* Laying - 1,944\n",
        "\n",
        "Participants/subjects (how many, and how split is done):\n",
        "* Participants: 30 volunteers aged between 19 and 48 years\n",
        "* Subjects - 30\n",
        "* Split - Subject independant split (training: 21 subjects, test: 9 subjects)\n",
        "\n",
        "Intended use / decision context:\n",
        "* Developing and evaluating machine learning algorithms to classify basic physical activities in real-time.\n",
        "* Context: Health monitoring and fitness tracking\n",
        "\n",
        "Non-intended use / known risks:\n",
        "* Not intended to detect concurrent activities, rather just one activity at a time.\n",
        "* The phone was strapped at the waist, so other data of watches may not work with this model\n",
        "* Elderly and young kids aren't in the age range for the data so they may not be represented well\n",
        "\n",
        "Preprocessing steps (normalization, filtering, windowing):\n",
        "* Filtering (Noise): A median filter and a 3rd order low-pass Butterworth filter with a 20 Hz cutoff were applied to remove high-frequency digital noise.\n",
        "\n",
        "* Gravity Separation: The acceleration signal was separated into Body Acceleration and Gravity using another low-pass Butterworth filter (cutoff of 0.3 Hz). Gravity is assumed to have only low-frequency components.\n",
        "\n",
        "* Windowing: The continuous stream was sliced into fixed-width sliding windows of 128 readings (2.56 seconds).\n",
        "\n",
        "* Overlap: A 50% overlap (64 samples) was used between windows to ensure transitional movements between activities weren't missed.\n",
        "\n",
        "Leakage risks + mitigation (subject split, normalization fit, duplicates):\n",
        "* Subject Split : The split is done by Subject IDs, the risk being that if Windows A and B are from the same person and have 50% overlap, the model will just remember the data. The mitigation can be to keep the same person's data in the same set (train or test)\n",
        "* Normalization fit: The risk is calculating the mean and stdev on the entire dataset. The mitigation is splitting the data and then calculating on the training data.\n",
        "* Duplicate Windows: The risk is that there can be overlaps and redundant information. The mitigation is to maintain the subject split."
      ],
      "metadata": {
        "id": "EyemKDXVbVZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qjkisqdabH9r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = '/kaggle/input/ucihar-dataset/UCI-HAR Dataset'\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "subject_train_file = os.path.join(path, 'train', 'subject_train.txt')\n",
        "subjects_train = np.loadtxt(subject_train_file, dtype=int)\n",
        "\n",
        "subjects_test_file = os.path.join(path, 'test', 'subject_test.txt')\n",
        "subjects_test = np.loadtxt(subjects_test_file, dtype=int)\n",
        "\n",
        "# Load raw sensor data\n",
        "body_acc_x = np.loadtxt(f'{path}/train/Inertial Signals/body_acc_x_train.txt')\n",
        "body_acc_y = np.loadtxt(f'{path}/train/Inertial Signals/body_acc_y_train.txt')\n",
        "body_acc_z = np.loadtxt(f'{path}/train/Inertial Signals/body_acc_z_train.txt')\n",
        "body_gyro_x = np.loadtxt(f'{path}/train/Inertial Signals/body_gyro_x_train.txt')\n",
        "body_gyro_y = np.loadtxt(f'{path}/train/Inertial Signals/body_gyro_y_train.txt')\n",
        "body_gyro_z = np.loadtxt(f'{path}/train/Inertial Signals/body_gyro_z_train.txt')\n",
        "\n",
        "# Stack features along the third dimension\n",
        "X_train = np.stack([body_acc_x, body_acc_y, body_acc_z, body_gyro_x, body_gyro_y, body_gyro_z], axis=2)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "\n",
        "y_train = np.loadtxt(f'{path}/train/y_train.txt')\n",
        "y_train = y_train - 1\n",
        "y_train = y_train.astype(int)\n",
        "\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "\n",
        "# Test data\n",
        "body_acc_x = np.loadtxt(f'{path}/test/Inertial Signals/body_acc_x_test.txt')\n",
        "body_acc_y = np.loadtxt(f'{path}/test/Inertial Signals/body_acc_y_test.txt')\n",
        "body_acc_z = np.loadtxt(f'{path}/test/Inertial Signals/body_acc_z_test.txt')\n",
        "body_gyro_x = np.loadtxt(f'{path}/test/Inertial Signals/body_gyro_x_test.txt')\n",
        "body_gyro_y = np.loadtxt(f'{path}/test/Inertial Signals/body_gyro_y_test.txt')\n",
        "body_gyro_z = np.loadtxt(f'{path}/test/Inertial Signals/body_gyro_z_test.txt')\n",
        "\n",
        "# Stack features along the third dimension\n",
        "X_test = np.stack([body_acc_x, body_acc_y, body_acc_z, body_gyro_x, body_gyro_y, body_gyro_z], axis=2)\n",
        "\n",
        "print(\"Testing data shape:\", X_test.shape)\n",
        "\n",
        "y_test = np.loadtxt(f'{path}/test/y_test.txt')\n",
        "y_test = y_test - 1\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "print(\"Training labels shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "3la0LnHEfSQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75b6fa2-d34d-4b33-fc86-df0f8be60043"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/ucihar-dataset/UCI-HAR Dataset\n",
            "Training data shape: (7352, 128, 6)\n",
            "Training labels shape: (7352,)\n",
            "Testing data shape: (2947, 128, 6)\n",
            "Training labels shape: (2947,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "3vrQBywifhja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_train.shape: (7352, 128, 6)\n",
        "- num_windows = 7352 (number of 2.56-second sliding windows in the train set)\n",
        "- T (window length / timesteps) = 128 (at 50 Hz sampling rate → 2.56 seconds per window)\n",
        "- C (channels) = 6 (x/y/z body acceleration + x/y/z body gyroscope)\n",
        "\n",
        "y_train.shape (post-remapping): (7352,)\n",
        "- Values are integers in the range 0 to 5\n",
        "\n",
        "\n",
        "X_test.shape: (2947, 128, 6)\n",
        "\n",
        "\n",
        "y_test.shape (2947,)"
      ],
      "metadata": {
        "id": "Ph5tHwuXpjJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activity_names = [\n",
        "    \"WALKING\",\n",
        "    \"WALKING_UPSTAIRS\",\n",
        "    \"WALKING_DOWNSTAIRS\",\n",
        "    \"SITTING\",\n",
        "    \"STANDING\",\n",
        "    \"LAYING\"\n",
        "]\n",
        "\n",
        "label_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "total_windows = len(y_train)\n",
        "percentages = (label_counts / total_windows * 100).round(2)\n",
        "\n",
        "df_dist = pd.DataFrame({\n",
        "    'Label': label_counts.index,\n",
        "    'Activity': [activity_names[i] for i in label_counts.index],\n",
        "    'Count': label_counts.values,\n",
        "    'Percentage': percentages.values.astype(str) + '%'\n",
        "})\n",
        "\n",
        "print(df_dist.to_string(index=False))"
      ],
      "metadata": {
        "id": "sw62vDgNfxRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d71bd0b-ad93-4074-a6aa-4ba9c8a91d75"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Label           Activity  Count Percentage\n",
            "     0            WALKING   1226     16.68%\n",
            "     1   WALKING_UPSTAIRS   1073     14.59%\n",
            "     2 WALKING_DOWNSTAIRS    986     13.41%\n",
            "     3            SITTING   1286     17.49%\n",
            "     4           STANDING   1374     18.69%\n",
            "     5             LAYING   1407     19.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaN in X_train:\", np.isnan(X_train).any())\n",
        "print(\"inf in X_train:\", np.isinf(X_train).any())\n",
        "print(\"NaN in y_train:\", np.isnan(y_train).any())"
      ],
      "metadata": {
        "id": "G0y-F3BJrvTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73af09ca-5656-4ad3-87ce-90ead6e8b937"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN in X_train: False\n",
            "inf in X_train: False\n",
            "NaN in y_train: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "rUURE3QafimK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(subjects_train) == len(X_train)\n",
        "\n",
        "unique_subjects = np.unique(subjects_train)\n",
        "\n",
        "train_subjects, val_subjects = train_test_split(\n",
        "    unique_subjects,\n",
        "    test_size=0.2,\n",
        "    random_state=45\n",
        ")\n",
        "\n",
        "\n",
        "train_mask = np.isin(subjects_train, train_subjects)\n",
        "val_mask  = np.isin(subjects_train, val_subjects)\n",
        "\n",
        "\n",
        "\n",
        "X_val  = X_train[val_mask]\n",
        "X_train = X_train[train_mask]\n",
        "\n",
        "\n",
        "y_val = y_train[val_mask]\n",
        "y_train = y_train[train_mask]\n",
        "\n",
        "subjects_train_split = subjects_train[train_mask]\n",
        "subjects_val_split  = subjects_train[val_mask]\n",
        "\n",
        "\n",
        "overlap = np.intersect1d(subjects_train_split, subjects_val_split)\n",
        "print(\"Subject overlap:\", len(overlap))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfz9Gm5Wsw5A",
        "outputId": "1226768e-8be3-4720-f985-6002fd0b3135"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject overlap: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4"
      ],
      "metadata": {
        "id": "g_kUu2LmfjqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find majority class in training set\n",
        "majority_class = np.bincount(y_train).argmax()\n",
        "print(\"Majority Class:\", majority_class)\n",
        "\n",
        "# Predict majority class for test set\n",
        "y_pred_majority = np.full_like(y_test, fill_value=majority_class)\n",
        "\n",
        "# Metrics\n",
        "macro_f1_majority = f1_score(y_test, y_pred_majority, average='macro')\n",
        "cm_majority = confusion_matrix(y_test, y_pred_majority)\n",
        "report_majority = classification_report(y_test, y_pred_majority, output_dict=True)\n",
        "per_class_f1_majority = {label: report_majority[str(label)]['f1-score']\n",
        "                         for label in report_majority.keys() if label.isdigit()}\n",
        "\n",
        "f1_table_majority = pd.DataFrame.from_dict(per_class_f1_majority, orient='index', columns=['F1-score'])\n",
        "\n",
        "print(\"\\nMajority-Class Predictor\")\n",
        "print(\"Macro-F1:\", macro_f1_majority)\n",
        "print(\"Confusion Matrix:\\n\", cm_majority)\n",
        "print(\"Per-Class F1:\\n\", f1_table_majority)\n"
      ],
      "metadata": {
        "id": "Ms2OFLvBqQF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedc2ff1-819f-40a4-92ea-490fd61d21c3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority Class: 5\n",
            "\n",
            "Majority-Class Predictor\n",
            "Macro-F1: 0.05137772675086108\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   0   0 496]\n",
            " [  0   0   0   0   0 471]\n",
            " [  0   0   0   0   0 420]\n",
            " [  0   0   0   0   0 491]\n",
            " [  0   0   0   0   0 532]\n",
            " [  0   0   0   0   0 537]]\n",
            "Per-Class F1:\n",
            "    F1-score\n",
            "0  0.000000\n",
            "1  0.000000\n",
            "2  0.000000\n",
            "3  0.000000\n",
            "4  0.000000\n",
            "5  0.308266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5"
      ],
      "metadata": {
        "id": "lGZrPL0Ffkas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_windows(X):\n",
        "    return X.reshape(X.shape[0], -1)\n",
        "\n",
        "X_train_f = flatten_windows(X_train)\n",
        "X_val_f   = flatten_windows(X_val)\n",
        "X_test_f  = flatten_windows(X_test)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "X_train_t = torch.tensor(X_train_f, dtype=torch.float32)\n",
        "Y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_val_t = torch.tensor(X_val_f, dtype=torch.float32)\n",
        "Y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "X_test_t = torch.tensor(X_test_f, dtype=torch.float32)\n",
        "Y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, Y_train_t), batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_val_t, Y_val_t), batch_size=batch_size)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_t, Y_test_t), batch_size=batch_size)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "input_dim = X_train_f.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "model = MLP(input_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "    return np.array(all_targets), np.array(all_preds)\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "best_val_f1 = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    y_val_true, y_val_pred = evaluate(val_loader)\n",
        "    val_macro_f1 = f1_score(y_val_true, y_val_pred, average='macro')\n",
        "\n",
        "    if val_macro_f1 > best_val_f1:\n",
        "        best_val_f1 = val_macro_f1\n",
        "        torch.save(model.state_dict(), \"best_mlp.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Val Macro-F1: {val_macro_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "OPmvtUp0oK_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860c8433-5b50-4aaf-e089-9e752e38292e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Val Macro-F1: 0.5955\n",
            "Epoch 2/30 | Val Macro-F1: 0.6401\n",
            "Epoch 3/30 | Val Macro-F1: 0.6350\n",
            "Epoch 4/30 | Val Macro-F1: 0.6911\n",
            "Epoch 5/30 | Val Macro-F1: 0.6754\n",
            "Epoch 6/30 | Val Macro-F1: 0.7048\n",
            "Epoch 7/30 | Val Macro-F1: 0.7354\n",
            "Epoch 8/30 | Val Macro-F1: 0.7051\n",
            "Epoch 9/30 | Val Macro-F1: 0.7186\n",
            "Epoch 10/30 | Val Macro-F1: 0.7776\n",
            "Epoch 11/30 | Val Macro-F1: 0.7768\n",
            "Epoch 12/30 | Val Macro-F1: 0.7796\n",
            "Epoch 13/30 | Val Macro-F1: 0.6966\n",
            "Epoch 14/30 | Val Macro-F1: 0.7915\n",
            "Epoch 15/30 | Val Macro-F1: 0.7598\n",
            "Epoch 16/30 | Val Macro-F1: 0.8068\n",
            "Epoch 17/30 | Val Macro-F1: 0.8160\n",
            "Epoch 18/30 | Val Macro-F1: 0.7642\n",
            "Epoch 19/30 | Val Macro-F1: 0.7348\n",
            "Epoch 20/30 | Val Macro-F1: 0.8280\n",
            "Epoch 21/30 | Val Macro-F1: 0.8279\n",
            "Epoch 22/30 | Val Macro-F1: 0.8005\n",
            "Epoch 23/30 | Val Macro-F1: 0.7484\n",
            "Epoch 24/30 | Val Macro-F1: 0.8316\n",
            "Epoch 25/30 | Val Macro-F1: 0.8035\n",
            "Epoch 26/30 | Val Macro-F1: 0.8362\n",
            "Epoch 27/30 | Val Macro-F1: 0.8416\n",
            "Epoch 28/30 | Val Macro-F1: 0.7966\n",
            "Epoch 29/30 | Val Macro-F1: 0.8389\n",
            "Epoch 30/30 | Val Macro-F1: 0.8428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6"
      ],
      "metadata": {
        "id": "o9uHS8kcflT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_mlp.pt\"))\n",
        "y_test_true, y_test_pred = evaluate(test_loader)\n",
        "\n",
        "# Primary Metric\n",
        "test_macro_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
        "print(\"\\nPrimary Metric:\")\n",
        "print(f\"Test Macro-F1: {test_macro_f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "report = classification_report(y_test_true, y_test_pred, output_dict=True)\n",
        "per_class_f1 = {\n",
        "    label: report[str(label)]['f1-score']\n",
        "    for label in report.keys()\n",
        "    if label.isdigit()\n",
        "}\n",
        "\n",
        "f1_table = pd.DataFrame.from_dict(per_class_f1, orient='index', columns=['F1-score'])\n",
        "print(\"\\nPer-Class F1 Scores:\")\n",
        "print(f1_table)\n"
      ],
      "metadata": {
        "id": "fofZ3lUCoOSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c4da2a-1c26-483e-e6ad-326400b0981d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Primary Metric:\n",
            "Test Macro-F1: 0.8326\n",
            "\n",
            "Confusion Matrix:\n",
            "[[447  17  27   2   1   2]\n",
            " [  1 461   9   0   0   0]\n",
            " [  0   5 414   0   1   0]\n",
            " [  0   0   0 392  63  36]\n",
            " [  2   3   0 154 334  39]\n",
            " [  0   0   0 121  34 382]]\n",
            "\n",
            "Per-Class F1 Scores:\n",
            "   F1-score\n",
            "0  0.945032\n",
            "1  0.963427\n",
            "2  0.951724\n",
            "3  0.675862\n",
            "4  0.692228\n",
            "5  0.767068\n"
          ]
        }
      ]
    }
  ]
}